# ARIA PRO Institutional Proxy Configuration
# Copy this to .env and edit with your actual values

# Local Ollama Configuration
LOCAL_OLLAMA_URL=http://127.0.0.1:11436

# Local Models (comma-separated list)
LOCAL_MODELS=mistral:latest,phi:latest,llama3:latest

# Remote GPT-OS Models
# Replace with your actual GPT-OS server URLs and API keys
GPTOS_120B_URL=https://your-gptos-server.com:8443/api/v1
GPTOS_120B_KEY=your_api_key_here

# Additional remote models (uncomment and configure as needed)
# GPTOS_70B_URL=https://another-gptos-server.com:8443/api/v1
# GPTOS_70B_KEY=another_api_key_here

# Proxy Configuration
PROXY_HOST=0.0.0.0
PROXY_PORT=11434

# Retry Configuration
MAX_RETRIES=3
RETRY_DELAY=1.0

# Caching
USE_CACHE=true
CACHE_SIZE=128
