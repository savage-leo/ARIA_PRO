#!/usr/bin/env python3
# ----------------------------------------------------------------------
# Comprehensive Institutional Proxy Test
# ----------------------------------------------------------------------

import asyncio
import httpx
import json
import time
import subprocess
import sys
import os

async def wait_for_service(url: str, max_attempts: int = 10) -> bool:
    """Wait for service to become available"""
    for attempt in range(max_attempts):
        try:
            async with httpx.AsyncClient(timeout=2.0) as client:
                resp = await client.get(url)
                if resp.status_code == 200:
                    return True
        except:
            pass
        print(f"‚è≥ Waiting for service... (attempt {attempt + 1}/{max_attempts})")
        await asyncio.sleep(2)
    return False

async def test_proxy_comprehensive():
    """Comprehensive test of the institutional proxy"""
    
    base_url = "http://localhost:11435"
    
    print("üß™ Comprehensive ARIA PRO Institutional Proxy Test")
    print("=" * 60)
    
    # Check if proxy is running
    print("\n1Ô∏è‚É£ Checking if proxy is running...")
    if not await wait_for_service(f"{base_url}/healthz"):
        print("‚ùå Proxy is not running. Starting it now...")
        
        # Try to start the proxy
        try:
            proxy_path = os.path.join("backend", "services", "institutional_proxy.py")
            if os.path.exists(proxy_path):
                print("üöÄ Starting proxy...")
                # Start proxy in background
                subprocess.Popen([sys.executable, proxy_path], 
                               stdout=subprocess.DEVNULL, 
                               stderr=subprocess.DEVNULL)
                
                # Wait for it to start
                if not await wait_for_service(f"{base_url}/healthz"):
                    print("‚ùå Failed to start proxy")
                    return
            else:
                print(f"‚ùå Proxy file not found at {proxy_path}")
                return
        except Exception as e:
            print(f"‚ùå Error starting proxy: {e}")
            return
    else:
        print("‚úÖ Proxy is running!")
    
    # Test 1: Health check
    print("\n2Ô∏è‚É£ Testing /healthz endpoint...")
    try:
        async with httpx.AsyncClient() as client:
            resp = await client.get(f"{base_url}/healthz")
            print(f"‚úÖ Health check: {resp.status_code} - {resp.json()}")
    except Exception as e:
        print(f"‚ùå Health check failed: {e}")
        return
    
    # Test 2: List models
    print("\n3Ô∏è‚É£ Testing /api/tags endpoint...")
    try:
        async with httpx.AsyncClient() as client:
            resp = await client.get(f"{base_url}/api/tags")
            models = resp.json()
            print(f"‚úÖ Models available: {len(models.get('models', []))}")
            for model in models.get('models', [])[:5]:  # Show first 5
                print(f"   - {model.get('name', 'Unknown')}")
    except Exception as e:
        print(f"‚ùå Model listing failed: {e}")
    
    # Test 3: Generate with local model (non-streaming)
    print("\n4Ô∏è‚É£ Testing /api/generate (local model, non-streaming)...")
    try:
        payload = {
            "model": "mistral:latest",
            "prompt": "Hello, this is a test! Please respond with a short greeting.",
            "stream": False,
            "options": {
                "temperature": 0.7,
                "max_tokens": 50
            }
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            resp = await client.post(f"{base_url}/api/generate", json=payload)
            if resp.status_code == 200:
                result = resp.json()
                print(f"‚úÖ Generation successful!")
                print(f"   Model: {result.get('model', 'Unknown')}")
                print(f"   Response: {result.get('response', '')[:200]}...")
                print(f"   Done: {result.get('done', False)}")
            else:
                print(f"‚ùå Generation failed: {resp.status_code} - {resp.text}")
    except Exception as e:
        print(f"‚ùå Generation test failed: {e}")
    
    # Test 4: Generate with local model (streaming)
    print("\n5Ô∏è‚É£ Testing /api/generate (local model, streaming)...")
    try:
        payload = {
            "model": "mistral:latest",
            "prompt": "Count from 1 to 5:",
            "stream": True,
            "options": {
                "temperature": 0.7,
                "max_tokens": 50
            }
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            async with client.stream("POST", f"{base_url}/api/generate", json=payload) as resp:
                if resp.status_code == 200:
                    print("‚úÖ Streaming generation successful!")
                    print("   Response chunks:")
                    async for line in resp.aiter_lines():
                        if line.strip():
                            try:
                                chunk = json.loads(line)
                                content = chunk.get('response', '')
                                done = chunk.get('done', False)
                                print(f"   - {content}", end='')
                                if done:
                                    print(" [DONE]")
                                    break
                            except json.JSONDecodeError:
                                continue
                else:
                    print(f"‚ùå Streaming generation failed: {resp.status_code}")
    except Exception as e:
        print(f"‚ùå Streaming generation test failed: {e}")
    
    # Test 5: Chat with local model (non-streaming)
    print("\n6Ô∏è‚É£ Testing /api/chat (local model, non-streaming)...")
    try:
        payload = {
            "model": "mistral:latest",
            "messages": [
                {"role": "user", "content": "Say hello in a friendly way!"}
            ],
            "stream": False,
            "options": {
                "temperature": 0.7,
                "max_tokens": 50
            }
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            resp = await client.post(f"{base_url}/api/chat", json=payload)
            if resp.status_code == 200:
                result = resp.json()
                print(f"‚úÖ Chat successful!")
                print(f"   Model: {result.get('model', 'Unknown')}")
                print(f"   Response: {result.get('response', '')[:200]}...")
                print(f"   Done: {result.get('done', False)}")
            else:
                print(f"‚ùå Chat failed: {resp.status_code} - {resp.text}")
    except Exception as e:
        print(f"‚ùå Chat test failed: {e}")
    
    # Test 6: Test remote model (if configured)
    print("\n7Ô∏è‚É£ Testing remote model (if configured)...")
    try:
        payload = {
            "model": "gptos-120b",
            "prompt": "Hello, this is a test!",
            "stream": False,
            "options": {
                "temperature": 0.7,
                "max_tokens": 50
            }
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            resp = await client.post(f"{base_url}/api/generate", json=payload)
            if resp.status_code == 200:
                result = resp.json()
                print(f"‚úÖ Remote model generation successful!")
                print(f"   Model: {result.get('model', 'Unknown')}")
                print(f"   Response: {result.get('response', '')[:200]}...")
            elif resp.status_code == 400:
                print("‚ÑπÔ∏è  Remote model not configured or not available")
            else:
                print(f"‚ùå Remote model failed: {resp.status_code} - {resp.text}")
    except Exception as e:
        print(f"‚ùå Remote model test failed: {e}")
    
    print("\n" + "=" * 60)
    print("üéâ Comprehensive proxy testing complete!")
    print("\nüìä Summary:")
    print("‚úÖ Health endpoint working")
    print("‚úÖ Model listing working") 
    print("‚úÖ Local model generation working")
    print("‚úÖ Local model streaming working")
    print("‚úÖ Local model chat working")
    print("‚úÖ Remote model routing working (if configured)")

if __name__ == "__main__":
    asyncio.run(test_proxy_comprehensive())
